---
title: "DDIA 笔记 | 03 存储和检索"
date: 2022-03-20
draft: false
---

![](http://ddia.vonng.com/img/ch3.png)

本章讨论如何将数据存入数据库，以及如何从数据库获取数据

面向事务以及面向分析的存储引擎大不相同。
讨论关系型数据库、NoSQL 数据库，以及两种存储引擎，log-structured 存储引擎和 page-oriented 存储引擎（例如 B-trees）

## 驱动数据库的数据结构

使用 bash 来实现数据库的写入、读取

```
#!/bin/bash
db_set () {
  echo "$1,$2" >> database
}

db_get () {
  grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

但是随着数据的增多，读取的性能会越来越糟糕，因为 db_get 操作会扫描整个数据库文件，算法复杂度是 O(n)，意味着如果数据量变为两倍，查询耗时也会变成两倍。

为了解决这个性能问题，我们需要另一种数据结构：索引（index）。索引就是额外保存一份元数据，作为路标来帮助更快地定位到需要的数据。

索引是额外的数据结构，并不会影响原始数据，仅会作用于查询速度。但是维持额外的数据结构会增加开销，尤其是写入，不管是什么类型的索引，都会减慢写入的速度，因为每次写入都需要更新索引。

所以对于存储系统而言，这是一个非常重要的 trade-off：选好索引可以加速查询，但是索引又回减慢写入。所以，数据库通常不会设置默认的索引，而是交给开发者或者 DBA 来选择。

### Hash 索引

key-value 数据的索引

k-v 存储有点像一些计算机语言中的字典（dictionary）类型，通常用 hash map 实现。

假设我们的数据存储只是针对一个文件在不停地追加内容。最简单可行的索引策略是：在内存中保存一个 hash map，每个 key 都映射了数据文件中的字节偏移，这个字节偏移就标识了值在文件中的位置，查询时，根据偏移能迅速定位，移动到对应扇区即可获取。如果往文件里新增一个健值对，就要更新 hash map 来映射最新写入的数据。

这个做法听起来简单，事实上，Riak 默认的存储引擎 Bitcask 就是这么做的。但是要注意的是，扫描一次磁盘就可以将 map 保存到内存中，要注意内存空间是否够用。而如果数据早就存在于文件缓存中，读取都不需要任何磁盘 I/O，相当高效。

像 Bitcask 这样的存储引擎，非常适合 value 高频更新的情况，例如 key 是猫猫视频的 url，value 是该视频的播放次数。在这种情况下，有大量的写入，但是没有太多唯一的 key，因此可以将所有的 key 保存在内存里。

但是文件不断地在追加内容，该怎么避免磁盘空间被用尽呢？一个好的做法是使用分段文档（segment file），当文档达到确定大小后，拆分出一个新的分段文档来写新的数据。我们可以对分段文档进行压缩，压缩就是丢弃 log 文件中重复的 key，仅保留最近更新的值。

压缩后的分段文件变得更小，同时也可以将多个分段文件合并在一起。合并、压缩的过程可以使用后台线程来跑，而不影响写入，完成后可以删除原有的分段文档。

每个分段文档都有一个在内存中的 hash 表，key 映射了文件的位置偏移，为了找到对应的值，得先查看最新分段文档的 hash map，如果没找到，再往前找。合并操作有效减少了分段文档的数量，所以通常不会有过多次的查询。

这套方法用到真实环境中，还有许多需要注意的地方：

* 文件格式
    * csv 不是文件的最好格式，二进制格式会更快、更简单
* 记录的删除
    * 如果想删除 key 和关联的 value，需要一个额外的文件来记录删除的数据（有时候称作为墓碑 tombstone）。合并分段文件时，用这个文件来判断是曾经已删除的 key
* 崩溃恢复
    * 如果数据库重启，内存中的 hash map 会丢失。原则上可以重新读取所有的分段文件来加载 hash map，如果文件很大，就会需要很长的时间，重启的时候就很痛苦。Bitcask 存储了 hash map 的镜像文件，来加速恢复。
* 部分写入的记录
    * 数据库随时都可能崩溃，可能存在写入了一半的记录。Bitcask 文件包含了校验和（checksums），来清楚、忽略这种记录。
* 并发控制
    * 为了保证写入顺序，常见的做法是单线程写入，因为是只追加并不可变的文件，又可以多线程读取。

只追加的文件看起来很浪费，但是设计上是为了解决以下场景：

* 追加和分段合并是顺序操作，通常而言会比随机写入更快，尤其是针对机械硬盘。某种程度上，顺序写入对于闪存固态硬盘而言也更好。
* 一致性和崩溃恢复会更简单。例如，不需要担心崩溃发生的时候，文件里有分散的新老数据。
* 合并老的文件来避免数据文件的碎片化

当然，hash table 索引也有局限性：

* hash table 要存在内存中，因此不能有大量的 key，而如果存在磁盘里，性能又不好，会有大量的随机 I/O 读取，hash 碰撞也需要复杂的逻辑来处理。
* 范围查询很低效，需要对每个 key 进行独立查询。

接下来我们会讨论不存在这些局限的索引结构。

### SSTables 和 LSM-Trees

Sorted String Table（SSTable），要求健值对按 key 排序，每个 key 在每个分段文件中只能出现一次。

和使用 hash 索引的日志分段相比，SSTable 有以下好处：
* 合并操作简单高效，类似于 mergesort 算法；如果多个分段里有相同的 key，取最新分段的就好（分段就是按时间片写入的）
* 查询某个 key 值时，不再需要把所有的 key 放在内存里，因为 key 排序后，保留部分分段的 key，能跳过不必要的查询
* 而正因为查询是按 key 的顺序来遍历，对应的数据也可以按顺序组织、压缩，然后写入磁盘里，内存里的 key 指向了这个顺序区块的开头，也能减少磁盘空间

#### 构建并维护 SSTable

写入的数据是无序的，那要如何保证 key 值的有序呢？

在磁盘里维护有序结构是困难的，但是在内存里就比较容易。有许多树结构可以使用，例如红黑树和 AVL 树。用这些树结构来保证任意顺序的写入，而有序读取。

所以存储引擎可以这样运行：
* 写入时，把数据存入内存中的平衡树结构（例如红黑树），这个树有时候称为内存表。
* 当内存树的大小超过阈值（一般是 MB 量级），就写入磁盘或者 SSTable 文件。树早就维护好了顺序，所以这一步很高效，写入的文件就是最新的分段，完成后就可以写入新的内存表
* 读请求时，先尝试从内存表获取，然后是最新的分段，再一路往前查询
* 后台线程可以进行合并、压缩操作，来合并分段表、擦除重复或者已删除的值

这个模式非常好用，不过还有一个问题：如果数据库崩溃了，最新的写入（内存表里的）会丢失，为了避免这个问题，可以保存一个写入日志，这个日志不需要排序，仅在崩溃恢复时用到。内存表写入 SSTable 后，关联的日志就可以删除。

#### 用 SSTable 制作 LSM-tree

上述的算法是 LevelDB 和 RocksDB 使用的技术，这些存储引擎被设计成可嵌入其他应用程序之中，例如 LevelDB 可代替 Riak 的 Bitcask。它们都受到了 Google 的 Bigtable 论文的启发（引入了术语 SSTable 和 memtable）

基于合并和压缩排序文件原理的存储引擎通常被称为 LSM 存储引擎。

Lucene 是 Elasticsearch 和 Solr 使用的一种全文搜索的索引引擎，它也是使用类似的方法来存储字典。全文索引会比 k-v 索引复杂，但是想法类似：根据一个词，找出文件中提到该词的所有内容（网页、描述等）。这是通过 k-v 结构实现的，key 是单词，value 是所有包含该词的文档 id 列表。在 Lucene 中，这些映射关系存储在类似于 SSTable 的文件中，并在后台按需合并。

#### 性能优化

想让存储引擎性能优异需要大量细节的设计。举个例子，LSM-tree 在查询不存在的 key 时，可能会变得很慢：你必须先检查内存表，然后检查从新到旧的分段，才能确定 key 不存在。为了优化这种查询，存储引擎通常会使用额外的布隆过滤器（Bloom Filters）。

还有一些不同的策略来决定 SSTable 的排序、被合并和压缩的时机。最常见的选择是 size-tiered 和 leveled compaction。size-tiered 是指较新、较小的 SSTable 会被合并入较旧、较大的 SSTable 中。leveled compaction 是指较旧的数据被移到单独的层级（level），使得压缩可以增量进行，节省磁盘开销。

LSM-tree 的核心思想：保存一系列在后台合并的 SSTable，简单又高效。即使数据集比内存大得多，也能很好地运行。而因为数据按序存储，可以高效地范围查询，又因为磁盘是顺序写入，LSM-tree 可以支持非常高的写入吞吐。

### B-Trees

目前使用最广的索引结构是 B-tree。

B-tree 也是健值对的形式，但是和 SSTable 有截然不同的设计哲学。

日志结构的索引将数据库拆分为可变大小的分段，通常为 MB 大小，并且按顺序写入；B-tree 则是将数据库分为固定大小的区块或者页，传统为 4KB 大小，一次只读写一页。这个设计更贴近底层的硬件，磁盘就是固定大小的区块。

每一页都有地址，页可以通过地址引用到另一页，有点像指针，但是指向的是磁盘位置而非内存。可以通过页之间的引用来组成页的树。有一页是树的根，每次查询都是从根开始。页中包含了许多 key 以及子页的引用。每一个字页都负责一定范围连续的 key，key 之间的差值表示了范围。

每一页中子页的引用数量被称为分支系数（branching factor），通常是数百的量级。

如果想要更新一个值，要找到存有 key 的页，更改 value，然后将页写回磁盘。如果想要新增一个 key，要找到 key 所在范围的页，然后加入 key。如果页中没有空间来保存新的 key，就会拆分成两个半页，父页也要更新 key 范围的计数。

这个算法保证了树的平衡：一个包含 n 个 key 的 B-tree，深度为 O(log n)。大部分数据库存入 B-tree 后，深度为 3 或者 4，所以不需要经过太多次的引用就能找到对应的页。（4 层、每页 4KB、分支系数为 500 的树可以存储 256TB 的数据）。

#### 可靠的 B-trees

覆盖页数据是硬件操作，而如果页分裂的话，一次需要多个磁盘旋转、写入操作，如果这时候数据崩溃就很麻烦。

为了保证数据库在崩溃时也可靠，B-tree 通常在磁盘里增加一个额外的数据结构：WAL（write-ahead-log，也被称为 redo log）。这个仅追加的文件保存了 B-tree 在真正更新页数据及其自身前的所有修改。当数据库在崩溃后启动，这个日志通常被用来使 B-tree 复位到一致状态。

还有另一个更新页面时的复杂情况，如果多个线程要同时访问 B-tree，则需要仔细控制并发，否则线程可能会看到不一致的树。通常使用 latches（轻量级锁）来保存树的数据结构。日志结构在这种情况下更简单，因为它们可以在后台进行合并，而不会影响查询，并可以时不时地将旧分段换成新分段。

#### B-tree 的优化

B-tree 以及存在很久了，所以有很多优化的设计，举几个例子：
* 一些数据库（如 LMDB）使用写时复制来替代 WAL 以支持崩溃恢复。修改的页面写到不同的位置，在树中创建父页面的新版本，指向这个新的位置。这个方法对于并发控制也很有用。
* 可以通过存储缩小后的健，来节省页面空间。特别是在树内部的页面上，健只需要提供足够的信息来充当范围之间的边界。在页面间保存更多的健，可以提升分支系数，从而减小层级（这个变体有时被称为 B+树，但这个优化已经被广泛使用，所以难以通过这点区分其他变体）
* 通常而言，页面可以放置在硬盘上的任何位置，并没有要求在磁盘上也按序存储。但是如果某个查询要按序扫描大范围的健，这种按页存储的布局可能会效率低下，因为每次页面读取都需要进行硬盘查找。因此，许多 B-tree 的实现会在布局树时尽量让叶子页面按序出现在硬盘上。但是，随着时间的推移，想要维持这个顺序是非常困难的。相比之下，LSM-tree 在合并过程中会重写大量分段，所以更容易在磁盘上保持有序。
* 在树中增加额外的指针。例如，每个叶子页面可以引用左边、右边的兄弟页面，不需要回跳父页面就可以按序扫描。
* B-tree 的变体，例如分形树（fractal tree），借用一些日志结构的思想来减少硬盘查找（实际上与分形无关）。
	
### 比较 B-Tree 和 LSM-Tree

单凭经验，LSM-tree 写更快，B-tree 读更快。

#### LSM-tree 的优势

B-tree 索引至少得写入两次数据，一次是预写日志，另一次是页本身，而且页更新时，整个页都得重新写入。

日志结构的索引，在重复压缩、合并 SSTable 时也会写入多次。数据库一次写入会导致磁盘的多次写入，这种情况被称为写扩增（write amplification）。使用 SSD 时要考虑这种情况，因为 SSD 在区块磨损前，写入次数是有限的。

对于重写入的应用，性能的瓶颈可能是数据库写入磁盘的效率，而写扩增会直接影响性能。

不仅如此，LSM-tree 通常比 B-tree 有更高的写入吞吐，一部分是因为有较少的写扩增，另一部分是因为它是顺序写入压缩的 SSTable 文件，而不是重写树中的页。对于机械硬盘比较重要，因为顺序写入比随机写入快得多。

LSM-tree 可以被很好地压缩，在磁盘上的文件也比 B-tree 小。B-tree 存储引擎会导致磁盘上有碎片的未使用空间：页分裂或者行无法写入已经存在的页面

#### LSM-tree 的劣势

日志结构的存储，一个劣势是合并操作有时候会干涉持续读写时的性能。磁盘资源是有限的，所以很容易产生这种情况，一个请求需要等待磁盘完成昂贵的合并操作。查询请求可能会有毛刺，相比之下，B-tree 更好预测。

高写入吞吐时，还有另一个问题：初始写入（写日志和把 memtable 写入磁盘）和后台的合并线程需要共享磁盘有限的写入带宽。写入一个空数据库时，全部的磁盘带宽可用于初始写入，而随着数据库增大，更多的带宽会用于合并。

如果写入吞吐很高，并且合并没有配置好，就很可能造成合并赶不上写入。没有合并的分段会持续扩张直到磁盘空间用尽，读取也会因为要查更多的分段而变慢。通常，基于 SSTable 的存储引擎不会限制写入率，如果合并跟不上，需要自己监控来避免。

B-tree 的一个优势是索引中的 key 存放在唯一的地方，而日志结构的存储引擎，相同的 key 可能存放在多个分段里。这一点可以让 B-tree 实现强事务语义：在很多关系型数据库中，事务隔离是通过锁一定范围的 key 实现的。

### 其他索引结构

#### 将值存在索引中

聚簇索引（clustered index）是将被索引的行直接存在索引中。例如，MySQL 的 InnoDB 存储引擎，表的主键总是一个聚簇索引，次级索引则引用主键（而不是堆文件中的位置）

在 聚簇索引（在索引中存储所有的行数据）和 非聚簇索引（仅在索引中存储对数据的引用）之间的折衷被称为 覆盖索引（covering index） 或 包含列的索引（index with included columns），其在索引内存储表的一部分列。这允许通过单独使用索引来处理一些查询（这种情况叫做：索引 覆盖（cover） 了查询）

#### 多列索引

最常见的多列索引被称为 连接索引（concatenated index） ，它通过将一列的值追加到另一列后面，简单地将多个字段组合成一个键（索引定义中指定了字段的连接顺序）。这就像一个老式的纸质电话簿，它提供了一个从（姓氏，名字）到电话号码的索引。由于排序顺序，索引可以用来查找所有具有特定姓氏的人，或所有具有特定姓氏 - 名字组合的人。但如果你想找到所有具有特定名字的人，这个索引是没有用的。（MySQL 的联合索引）

多维索引（multi-dimensional index） 是一种查询多个列的更一般的方法，这对于地理空间数据尤为重要。例如，餐厅搜索网站可能有一个数据库，其中包含每个餐厅的经度和纬度。

#### 全文搜索和模糊索引

全文搜索引擎通常允许搜索一个单词以扩展为包括该单词的同义词，忽略单词的语法变体，搜索在相同文档中彼此靠近的单词的出现，并且支持各种其他取决于文本的语言分析功能。为了处理文档或查询中的拼写错误，Lucene 能够在一定的编辑距离（编辑距离 1 意味着添加，删除或替换了一个字母）内搜索文本。

#### 在内存中存储一切

硬盘有两个优点：持久化（关闭电源时内容不会丢），每 GB 的成本比 RAM 低。
但是随着 RAM 变得便宜，每 GB 的成本就不再是优势。如果数据量不大，把它们全部保存在内存中是非常可行的，这也造就了内存数据库的发展。

某些内存中的键值存储（如 Memcached）仅用于缓存，在重启时丢失数据也是可以接受的。另一些内存数据库的目标是持久性，可以通过特殊的硬件（供电的 RAM）来实现，也可以将更改日志写入硬盘，或者将快照定时写入硬盘、拷贝状态到其他机器上。

内存数据库重新启动时，需要从硬盘或通过网络从副本重新加载其状态（除非使用特殊的硬件）。尽管写入硬盘，它仍然是一个内存数据库，因为硬盘仅出于持久性目的进行日志追加，读取请求完全由内存来处理。写入硬盘同时还有运维上的好外：硬盘上的文件可以很容易地由外部实用程序进行备份、检查和分析。

反直觉的是，内存数据库的性能优势并不是因为它们不需要从硬盘读取的事实。只要有足够的内存即使是基于硬盘的存储引擎也可能永远不需要从硬盘读取，因为操作系统在内存中缓存了最近使用的硬盘块。相反，它们更快的原因在于省去了将内存数据结构编码为硬盘数据结构的开销。

除了性能，内存数据库的另一个有趣的地方是提供了难以用基于硬盘的索引实现的数据模型。例如，Redis 为各种数据结构（如优先级队列和集合）提供了类似数据库的接口。因为它将所有数据保存在内存中，所以它的实现相对简单。

最近的研究表明，内存数据库体系结构可以扩展到支持比可用内存更大的数据集，而不必重新采用以硬盘为中心的体系结构。所谓的 反缓存（anti-caching） 方法通过在内存不足的情况下将最近最少使用的数据从内存转移到硬盘，并在将来再次访问时将其重新加载到内存中。这与操作系统对虚拟内存和交换文件的操作类似，但数据库可以比操作系统更有效地管理内存，因为它可以按单个记录的粒度工作，而不是整个内存页面。尽管如此，这种方法仍然需要索引能完全放入内存中（就像本章开头的 Bitcask 例子）。

如果 非易失性存储器（non-volatile memory, NVM） 技术得到更广泛的应用，可能还需要进一步改变存储引擎设计。目前这是一个新的研究领域，值得关注。

## 事务处理还是分析？

比较事务处理和分析系统的特点：

| 属性 | 事务处理系统 OLTP | 分析系统 OLAP |
|  ----  | ----  | ----  |
| 主要读取模式 | 查询少量记录，按键读取 | 在大批量记录上聚合 |
| 主要写入模式 | 随机访问，写入要求低延时 | 批量导入（ETL）或者事件流 |
| 主要用户 | 终端用户，通过 Web 应用 | 内部数据分析师，用于决策支持 |
| 处理的数据 | 数据的最新状态（当前时间点） | 随时间推移的历史事件 |
| 数据集大小 | GB ~ TB | TB ~ PB |

起初，事务处理和分析查询使用了相同的数据库。 SQL 在这方面已证明是非常灵活的：对于 OLTP 类型的查询以及 OLAP 类型的查询来说效果都很好。尽管如此，在 80s 末和 90s 初期，企业有停止使用 OLTP 系统进行分析的趋势，转而在单独的数据库上运行分析。这个单独的数据库被称为 数据仓库（data warehouse）。

### 数据仓库

OLTP 系统往往对于业务运作至关重要，因此通常会要求高可用与低延迟。DBA 也不愿意让业务分析人员在 OLTP 数据库上运行临时的分析查询，因为这些查询开销巨大，会扫描大部分数据集，这会损害同时在执行的事务的性能。

相比之下，数据仓库是一个独立的数据库，包含各种 OLTP 系统中只读数据的副本。从 OLTP 数据库中提取数据（定期的数据转储或连续的更新流），转换成适合分析的模式，清理并加载到数据仓库中，这个过程称为“抽取 - 转换 - 加载（ETL）”

#### OLTP 数据库和数据仓库的分歧

数据仓库的数据模型通常是关系型的，因为 SQL 通常很适合分析查询。有许多图形数据分析工具可以生成 SQL 查询，可视化结果，并允许分析人员探索数据（通过下钻、切片和切块等操作）。

表面上，一个数据仓库和一个关系型 OLTP 数据库看起来很相似，因为它们都有一个 SQL 查询接口。然而，系统的内部看起来可能完全不同，因为它们针对非常不同的查询模式进行了优化。现在许多数据库供应商都只是重点支持事务处理负载和分析工作负载这两者中的一个，而不是都支持。

一些数据库（例如 Microsoft SQL Server 和 SAP HANA）支持在同一产品中进行事务处理和数据仓库。但是，它们也正日益成为两个独立的存储和查询引擎，只是这些引擎正好可以通过一个通用的 SQL 接口访问。

### 分析的模式：星型和雪花型

在 OLTP 中有许多数据模型，但是在 OLAP 中，数据模型就很少。许多数据仓库使用公式化的模型，被称为星型模型（或维度模型）。

事实表（fact table）的每一行代表了在特定时间发生的事件，事实表中的一些列是属性，另一些是对其他表（称为维度表 dimension table）的外键引用，维度代表了事件的对象、内容、地点、时间、方式和原因（who, what, where, when, how, and why）。

“星型模型”（star schema）的名字来源于将这些关系可视化后，事实表在中间，被维度表所包围，这些表的连接就像星星的光芒。

这个模式的变体是雪花模式（snowflake schema），其中维度被进一步拆分为子维度。雪花模式比星型模式更规范，但是星型模式是首选，因为用起来简单。

## 列式存储

如果事实表中有万亿行和数 PB 的数据，那么高效的存储和查询将会是一个巨大的挑战。

事实表通常有超过 100 个的列，但是典型的数仓查询只会访问其中的 4 个或 5 个列，如何高效地查询呢？

在大多数 OLTP 数据库中，存储都是按行布局的：表格中的每行都会相邻存储。文档型数据库也是一样，整个文档通常存储为一个连续的字节序列。面向行的存储引擎需要把这些行（每行包含超过 100 个字段）从硬盘加载到内存中，解析然后再过滤不需要的属性，会浪费很多时间。

所以列式存储背后的思想很简单：不是将行内的数据存在一起，而是将来自每一列的所有值存在一起。如果每个列存在单独的文件中，查询只需要读取需要的列，可以节省大量的工作。

列式存储布局依赖于每个列文件包含相同顺序的行，如果需要组装成完整的行，可以从每个单独的文件中获取第 23 行，组合起来形成表中的第 23 行。

### 列压缩

除了仅从硬盘加载查询所需的列以外，我们还可以通过压缩数据来进一步降低对硬盘吞吐量的需求。而且，列式存储通常很适合压缩。

根据列中的数据，可以使用不同的压缩技术。在数据仓库中特别有效的是位图编码。

### 列式存储中的排序

在列式存储中，存储行的顺序不是很重要。按插入顺序来存储是最简单的，因为每插入一行只需要在每个列文件追加就好。不过，我们可以选择增加一个特定的顺序，就像之前对 SSTable 做的那样，并将其用作索引机制。

注意，每列独自排序是没有意义的，因为那样我们就没法知道不同列中的哪些项属于同一行。我们只能在知道一列中的第 k 项与另一列中的第 k 项属于同一行的情况才能重建出完整的行。

排序顺序的另一个好处是它可以帮助压缩列。第一个排序键的压缩效果最强。第二和第三个排序键会更混乱，因此不会有这么长的连续的重复值。排序优先级更低的列以基本上随机的顺序出现，所以它们可能不会被压缩。但前几列排序在整体上仍然是有好处的。

### 写入列式存储

列式存储、压缩和排序都有助于加速读取用于分析的数据，但缺点是写入更困难。

对于压缩的列，如果想在有序表的中间增加一行，很可能得重写所有的列文件，插入必须对所有列进行一致的更新。

前面已经讲述了一个很好的解决方案：LSM-tree。所有的写操作先写入内存表，此时被添加到一个已排序的结构中，并准备写入磁盘。而内存中是行存储还是列存储并不重要，当积累了足够多的数据后，它们将与磁盘中的列文件合并，批量写入新文件中，Vertica 基本上就是这么做的。

查询需要检查硬盘上的列数据和最近在内存中的写入，并将两者结合起来。但是，查询优化器对用户隐藏了这个细节。从分析师的角度来看，通过插入、更新或删除操作进行修改的数据会立即反映在后续的查询中。

### 聚合：数据立方和物化试图

数据仓库的另一个值得一提的方面是物化汇总（materialized aggregates）。如前所述，数据仓库查询通常涉及一个聚合函数，如 SQL 中的 COUNT、SUM、AVG、MIN 或 MAX。如果相同的聚合被许多不同的查询使用，那么每次都通过原始数据来处理可能太浪费了。为什么不将一些查询使用最频繁的计数或总和缓存起来？

创建这种缓存的一种方式是物化视图（Materialized View）。在关系数据模型中，它通常被定义为一个标准（虚拟）视图：一个类似于表的对象，其内容是一些查询的结果。不同的是，物化视图是查询结果的实际副本，会被写入硬盘，而虚拟视图只是编写查询的一个捷径。从虚拟视图读取时，SQL 引擎会将其展开到视图的底层查询中，然后再处理展开的查询。

当底层数据发生变化时，物化视图需要更新，因为它是数据的非规范化副本。数据库可以自动完成该操作，但是这样的更新使得写入成本更高，这就是在 OLTP 数据库中不经常使用物化视图的原因。在读取繁重的数据仓库中，它们可能更有意义（它们是否实际上改善了读取性能取决于个别情况）。

物化视图的常见特例称为数据立方体或 OLAP 立方。

物化数据立方体的优点是可以让某些查询变得非常快，因为它们已经被有效地预先计算了。例如，如果你想知道每个商店的总销售额，则只需查看合适维度的总计，而无需扫描数百万行的原始数据。

数据立方体的缺点是不具有查询原始数据的灵活性。例如，没有办法计算有多少比例的销售来自成本超过 100 美元的项目，因为价格不是其中的一个维度。因此，大多数数据仓库试图保留尽可能多的原始数据，并将聚合数据（如数据立方体）仅用作某些查询的性能提升手段。

## 总结

在本章中，我们试图深入了解数据库是如何处理存储和检索的。将数据存储在数据库中会发生什么？稍后再次查询数据时数据库会做什么？

在高层次上，我们看到存储引擎分为两大类：针对 事务处理（OLTP） 优化的存储引擎和针对 在线分析（OLAP） 优化的存储引擎。这两类使用场景的访问模式之间有很大的区别：

OLTP 系统通常面向最终用户，这意味着系统可能会收到大量的请求。为了处理负载，应用程序在每个查询中通常只访问少量的记录。应用程序使用某种键来请求记录，存储引擎使用索引来查找所请求的键的数据。硬盘查找时间往往是这里的瓶颈。
数据仓库和类似的分析系统会低调一些，因为它们主要由业务分析人员使用，而不是最终用户。它们的查询量要比 OLTP 系统少得多，但通常每个查询开销高昂，需要在短时间内扫描数百万条记录。硬盘带宽（而不是查找时间）往往是瓶颈，列式存储是针对这种工作负载的日益流行的解决方案。
在 OLTP 这一边，我们能看到两派主流的存储引擎：

日志结构学派：只允许追加到文件和删除过时的文件，但不会更新已经写入的文件。Bitcask、SSTables、LSM 树、LevelDB、Cassandra、HBase、Lucene 等都属于这个类别。
就地更新学派：将硬盘视为一组可以覆写的固定大小的页面。 B 树是这种理念的典范，用在所有主要的关系数据库和许多非关系型数据库中。
日志结构的存储引擎是相对较新的技术。他们的主要想法是，通过系统性地将随机访问写入转换为硬盘上的顺序写入，由于硬盘驱动器和固态硬盘的性能特点，可以实现更高的写入吞吐量。

关于 OLTP，我们最后还介绍了一些更复杂的索引结构，以及针对所有数据都放在内存里而优化的数据库。

然后，我们暂时放下了存储引擎的内部细节，查看了典型数据仓库的高级架构，并说明了为什么分析工作负载与 OLTP 差别很大：当你的查询需要在大量行中顺序扫描时，索引的重要性就会降低很多。相反，非常紧凑地编码数据变得非常重要，以最大限度地减少查询需要从硬盘读取的数据量。我们讨论了列式存储如何帮助实现这一目标。

作为一名应用程序开发人员，如果你掌握了有关存储引擎内部的知识，那么你就能更好地了解哪种工具最适合你的特定应用程序。如果你需要调整数据库的调整参数，这种理解可以让你设想一个更高或更低的值可能会产生什么效果。

尽管本章不能让你成为一个特定存储引擎的调参专家，但它至少大概率使你有了足够的概念与词汇储备去读懂你所选择的数据库的文档。
